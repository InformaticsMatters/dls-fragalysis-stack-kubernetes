---

# The media-replicator provides the ability to replicate files from one volume
# to another, where one volume is for a PVC in one cluster and the other a
# static volume available to other clusters. So data can be replicated out of
# one cluster (onto the static volume) and into another (from the static
# volume).
#
# In this role the static volume is mounted into the Pod as 'volume-a'
# and the (dynamic) PVC as 'volume-b'. The replicator then accepts
# a 'REPLICATE_DIRECTION' environment variable that is either
# 'AtoB' or 'BtoA'. So, to copy the data from a PVC in a namespace to NFS
# (i.e. essentially a 'backup') the direction would be 'BtoA`.
# To recover NFS data to a namespace PVC the direction would be 'AtoB'.
#
# The replica Pod employs 'rsync' where '--delete' is used to ensure
# the destination is a copy of the source (with extraneous files removed).
# This can be controlled by the 'REPLICATE_DELETE' Pod environment variable.
#
# We provide Job and CronJob templates - for single-shot and
# recurring replication where the default role behaviour (i.e. via main)
# id to run a 'single-shot' replication.

# Set one of 'developer stack name' (typically 'default')
# or 'production stack namespace'.
#
# The replica direction is based on which variable is set.
# If 'stack_name' is used like 'default' (assumed to be a developer stack's name)
# the 'stack_ns' must not be used and the direction will be 'AtoB'.
# If the 'stack_ns' is used it must be one of 'production-stack' ('BtoA') or
# 'staging-stack' ('AtoB'). This way you cannot write to the production volume,
# but you are forced to write to the staging volume.
replicator_stack_name: ''
replicator_stack_ns: ''

# Details of the statically assigned replica volume (on an NFS server)
# (volume-a)
replicator_a_nfs_server: 130.246.213.182
replicator_a_nfs_path: /nfs/kubernetes-media-replica
# Details of the PVC to copy from or copy into (based on direction)
# (volume-b). Typically 'media' for a stack.
replicator_b_pvc_name: media

# If a schedule is set the playbook will use the following
# schedule in the 'ConJob' template - i.e. 21 minutes past each hour.
# Replication can take a significant time - so ensure you're not
# running this too often!
replicator_schedule: '21 * * * *'

# Decrease log for replication?
replicate_quietly: yes

# If 'replicator_schedule' is blank ('') the playbook will use this as the
# length of time to wait for the replicator 'Job' to complete.
# But expect replication of production media data (say around 4.5Gi)
# to take at least 10 minutes to complete over NFS.
replicator_job_timeout_minutes: 90
