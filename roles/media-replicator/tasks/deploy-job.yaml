---

# Remove any existing replicator Job,
# and remove any stopped (Succeeded) Jobs (now Pods)
# before starting a new replication...

- name: Remove exising replicator Job
  k8s:
    state: absent
    definition: "{{ lookup('template', 'job.yaml.j2') }}"
    wait: yes

# Delete the Pods left behind that are Completed ('Succeeded').
# Kubernetes *DOES NOT* remove these Job-based Pods automatically
# (see https://kubernetes.io/docs/concepts/workloads/controllers/job/).
# Instead completed Jobs need to be removed manually by the user.
# The logic that follows list all NFS replicator Jobs (Pods)
# that have Succeeded and then deletes them.

- name: Get Succeeded replicator Pods
  k8s_info:
    kind: Pod
    namespace: "{{ stack_namespace_fact }}"
    label_selectors:
    - name=nfs-replicator
    field_selectors:
    - status.phase=Succeeded
  register: pods_result

- name: Delete Succeeded replicator Pods
  k8s:
    kind: Pod
    namespace: "{{ stack_namespace_fact }}"
    name: "{{ item.metadata.name }}"
    state: absent
  loop: "{{ pods_result.resources }}"
  when: pods_result.resources|length > 0

# All clean - start a new Job

- name: Launching new replicator Job
  k8s:
    definition: "{{ lookup('template', 'job.yaml.j2') }}"
    wait: yes

# Wait for the replicator to complete

- name: Wait for replicator Job ({{ replicator_job_timeout_minutes }} minutes)
  k8s_info:
    kind: Job
    namespace: "{{ stack_namespace_fact }}"
    name: nfs-replicator
  register: result
  until: >-
    result.resources[0].status.completionTime is defined
    or result.resources[0].status.failed is defined
  delay: 60
  retries: "{{ (replicator_job_timeout_minutes|int * 60 / 60)|int }}"

- name: Display replicator Job status
  debug:
    var: result.resources[0].status

- name: Assert replicator Job success
  assert:
    that:
    - result.resources[0].status.succeeded is defined
    - result.resources[0].status.succeeded == 1
    - result.resources[0].status.failed is not defined
